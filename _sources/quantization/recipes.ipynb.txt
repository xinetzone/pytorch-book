{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 量化菜谱\n",
    "\n",
    "{guilabel}`参考`：[量化菜谱](https://pytorch.org/tutorials/recipes/quantization.html)\n",
    "\n",
    "量化是一种将模型参数中的 32 位浮点数转换为 8 位整数的技术。通过量化，模型大小和内存占用可以减少到原来大小的 1/4，推理速度可以提高 2-4 倍，而精度保持不变。\n",
    "\n",
    "量化模型的方法或工作流程大致有三种：后训练动态量化（post training dynamic quantization）、后训练静态量化（post training dynamic quantization）和量化感知训练（quantization aware training）。但是，如果您想要使用的模型已经有量化版本，您可以直接使用它，而不需要经过上面三个工作流中的任何一个。例如，`torchvision` 库已经包括了模型 MobileNet v2、ResNet 18、ResNet 50、Inception v3、GoogleNet 等的量化版本。因此，我们将最后一种方法作为另一个工作流，尽管很简单。\n",
    "\n",
    "```{note}\n",
    "量化支持可用于[有限的一组运算符](https://pytorch.org/blog/introduction-to-quantization-on-pytorch/#device-and-operator-support)。\n",
    "```\n",
    "\n",
    "PyTorch 支持四种量化工作流。\n",
    "\n",
    "## 使用预先训练的量化 MobileNet v2\n",
    "\n",
    "要得到 MobileNet v2 量化模型，只需做："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pc/xinet/anaconda3/envs/torchx/lib/python3.10/site-packages/torch/ao/quantization/utils.py:210: UserWarning: must run observer before calling calculate_qparams. Returning default values.\n",
      "  warnings.warn(\n",
      "[W TensorImpl.h:1463] Warning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (function operator())\n"
     ]
    }
   ],
   "source": [
    "from torchvision.models.quantization import mobilenet_v2 as qmobilenet_v2\n",
    "\n",
    "model_quantized = qmobilenet_v2(pretrained=True, quantize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了比较非量化的 MobileNet v2 模型与其量化版本的大小差异："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/mobilenet_v2-b0353104.pth\" to /home/pc/.cache/torch/hub/checkpoints/mobilenet_v2-b0353104.pth\n",
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.26 MB\n",
      "3.63 MB\n"
     ]
    }
   ],
   "source": [
    "from torchvision.models import mobilenet_v2\n",
    "\n",
    "model = mobilenet_v2(pretrained=True)\n",
    "\n",
    "import os\n",
    "import torch\n",
    "\n",
    "def print_model_size(mdl):\n",
    "    torch.save(mdl.state_dict(), \"tmp.pt\")\n",
    "    print(\"%.2f MB\" %(os.path.getsize(\"tmp.pt\")/1e6))\n",
    "    os.remove('tmp.pt')\n",
    "\n",
    "print_model_size(model)\n",
    "print_model_size(model_quantized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 后训练动态量化\n",
    "\n",
    "要应用动态量化，它将模型中的所有权重从 32 位的浮点数转换为 8 位的整数，但在对这些激活执行计算之前不会将激活转换为 int8，只需调用torque . quantized .quantize_dynamic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.ao.quantization.quantize import quantize_dynamic\n",
    "\n",
    "model_dynamic_quantized = quantize_dynamic(\n",
    "    model, qconfig_spec={torch.nn.Linear}, dtype=torch.qint8\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中 `qconfig_spec` 指定要应用量化的 `model` 中的子模块名称列表。\n",
    "\n",
    "```{warning}\n",
    "动态量化的一个重要限制是，虽然它是最简单的工作流程，如果你没有预先训练的量化模型准备使用，它目前在 `qconfig_spec` 中只支持 `nn.Linear` 和 `nn.LSTM`，这意味着你将不得不使用静态量化或量化感知训练，稍后讨论，以量化其他模块，如 `nn.Conv2d`。\n",
    "```\n",
    "\n",
    "`quantize_dynamic` API 调用的完整文档在 {mod}`~torch.ao.quantization.quantize.quantize_dynamic`。使用训练后动态量化的另外三个例子是 [Bert 例子](https://pytorch.org/tutorials/intermediate/dynamic_quantization_bert_tutorial.html)、[LSTM 模型例子](https://pytorch.org/tutorials/advanced/dynamic_quantization_tutorial.html#test-dynamic-quantization) 和另一个 [LSTM demo 例子](https://pytorch.org/tutorials/recipes/recipes/dynamic_quantization.html#do-the-quantization)。"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "78526419bf48930935ba7e23437b2460cb231485716b036ebb8701887a294fa8"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 ('torchx')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
