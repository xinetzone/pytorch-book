
<!DOCTYPE html>

<html lang="zh_CN">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>自动微分 &#8212; Pytorch Book 0.0.1 文档</title>
    
  <link href="../../_static/css/theme.css" rel="stylesheet">
  <link href="../../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/css/style.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/tabs.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/default.css" />
    
  <link rel="preload" as="script" href="../../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js"></script>
    <script src="../../_static/translations.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="canonical" href="https://xinetzone.github.io/pytorch-book/tutorials/basics/autogradqs.html" />
    <link rel="shortcut icon" href="../../_static/favicon.jpg"/>
    <link rel="index" title="索引" href="../../genindex.html" />
    <link rel="search" title="搜索" href="../../search.html" />
    <link rel="next" title="笔记" href="../notes/index.html" />
    <link rel="prev" title="快速入门" href="quickstart.html" />
    <link rel="stylesheet" href="../../_static/css/default.css"/>
    <link rel="stylesheet" href="../../_static/css/custom.css"/>

  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    
    <nav class="navbar navbar-light navbar-expand-lg bg-light fixed-top bd-navbar" id="navbar-main"><div class="container-xl">

  <div id="navbar-start">
    
    

<a class="navbar-brand" href="../../index.html">
  <img src="../../_static/logo.jpg" class="logo" alt="logo">
</a>


    
  </div>

  <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbar-collapsible" aria-controls="navbar-collapsible" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
  </button>

  
  <div id="navbar-collapsible" class="col-lg-9 collapse navbar-collapse">
    <div id="navbar-center" class="mr-auto">
      
      <div class="navbar-center-item">
        <ul id="navbar-main-elements" class="navbar-nav">
    <li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../../intro.html">
  项目简介
 </a>
</li>

<li class="toctree-l1 current active nav-item">
 <a class="reference internal nav-link" href="../index.html">
  教程
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../../quantization/index.html">
  量化
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../../ecosystem/index.html">
  生态系统
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../../refs.html">
  参考文献
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../../others/index.html">
  其他
 </a>
</li>

    
    <li class="nav-item">
        <a class="nav-link nav-external" href="https://xinetzone.github.io/pytorch-book/api">API<i class="fas fa-external-link-alt"></i></a>
    </li>
    
</ul>
      </div>
      
    </div>

    <div id="navbar-end">
      
      <div class="navbar-end-item">
        <ul id="navbar-icon-links" class="navbar-nav" aria-label="Icon Links">
        <li class="nav-item">
          <a class="nav-link" href="https://github.com/xinetzone/pytorch-book" rel="noopener" target="_blank" title="GitHub">
            <span><i class="fab fa-github-square"></i></span>
            <label class="sr-only">GitHub</label>
          </a>
        </li>
      </ul>
      </div>
      
    </div>
  </div>
</div>
    </nav>
    

    <div class="container-xl">
      <div class="row">
          
            
            <!-- Only show if we have sidebars configured, else just a small margin  -->
            <div class="col-12 col-md-3 bd-sidebar"><form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
  <div class="bd-toc-item active">
    <ul class="current nav bd-sidenav">
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="index.html">
   PyTorch 基础
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2">
    <a class="reference internal" href="quickstart.html">
     快速入门
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     自动微分
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../notes/index.html">
   笔记
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../notes/autograd-mechanics.html">
     自动微分机制
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../notes/extending.html">
     扩展 PyTorch
    </a>
   </li>
  </ul>
 </li>
</ul>

  </div>
</nav>
            </div>
            
          

          
          <div class="d-none d-xl-block col-xl-2 bd-toc">
            
              
              <div class="toc-item">
                
<div class="tocsection onthispage pt-5 pb-3">
    <i class="fas fa-list"></i> On this page
</div>

<nav id="bd-toc-nav">
    <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id2">
   张量、函数和计算图
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id3">
   计算梯度
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id4">
   禁用梯度追踪
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id5">
   更多关于计算图的内容
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id6">
   选读：张量梯度和雅可比积
  </a>
 </li>
</ul>

</nav>
              </div>
              
              <div class="toc-item">
                

<div class="tocsection editthispage">
    <a href="https://github.com/xinetzone/pytorch-book/edit/main/docs/tutorials/basics/autogradqs.ipynb">
        <i class="fas fa-pencil-alt"></i> Edit this page
    </a>
</div>

              </div>
              
            
          </div>
          

          
          
            
          
          <main class="col-12 col-md-9 col-xl-7 py-md-5 pl-md-5 pr-md-4 bd-content" role="main">
              
              <div>
                
  <div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%</span><span class="k">matplotlib</span> inline
</pre></div>
</div>
</div>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="id1">
<h1>自动微分<a class="headerlink" href="#id1" title="永久链接至标题">¶</a></h1>
<p><span class="guilabel">参考</span>：<a class="reference external" href="https://pytorch.org/tutorials/beginner/basics/autogradqs_tutorial.html">autogradqs</a></p>
<p>在训练神经网络时，最常用的算法是 <strong>反向传播</strong> （back propagation）。在该算法中，参数（模型权值）根据损失函数相对于给定参数的 <strong>梯度</strong> （gradient）进行调整。</p>
<p>为了计算这些梯度，PyTorch 内置了名为 <a class="reference external" href="https://pytorch.org/docs/stable/autograd.html#module-torch.autograd" title="(在 PyTorch v1.11.0)"><code class="xref py py-mod docutils literal notranslate"><span class="pre">torch.autograd</span></code></a> 的微分引擎。它对任何计算图，支持自动计算梯度。</p>
<p>考虑最简单的单层神经网络，输入 <code class="docutils literal notranslate"><span class="pre">x</span></code>，参数 <code class="docutils literal notranslate"><span class="pre">w</span></code> 和 <code class="docutils literal notranslate"><span class="pre">b</span></code>，以及一些损失函数。它可以在 PyTorch 中以如下方式定义：</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>  <span class="c1"># 输入 tensor</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>  <span class="c1"># 期望的 output</span>
<span class="n">w</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span><span class="o">+</span><span class="n">b</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">binary_cross_entropy_with_logits</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="id2">
<h2>张量、函数和计算图<a class="headerlink" href="#id2" title="永久链接至标题">¶</a></h2>
<p>上面的代码定义了如下计算图：</p>
<div class="figure align-default">
<img alt="../../_images/comp-graph.png" src="../../_images/comp-graph.png" />
</div>
<p>在这个网络中，<code class="docutils literal notranslate"><span class="pre">w</span></code> 和 <code class="docutils literal notranslate"><span class="pre">b</span></code> 是需要优化的参数。因此，需要能够计算相对于这些变量的损失函数的梯度。为了做到这一点，设置这些张量的 <code class="docutils literal notranslate"><span class="pre">requires_grad</span></code> 属性。</p>
<div class="alert-info admonition note">
<p class="admonition-title">备注</p>
<p>可以在创建张量时设置 <code class="docutils literal notranslate"><span class="pre">requires_grad</span></code> 的值，或者稍后使用 <code class="docutils literal notranslate"><span class="pre">x.requires_grad_(True)</span></code> 方法。</p>
</div>
<p>应用在张量上构造计算图的函数实际上是 <a class="reference external" href="https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function" title="(在 PyTorch v1.11.0)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Function</span></code></a> 类的对象。该对象知道如何在前向过程计算损失，也知道如何在反向传播步骤中计算其导数。对向后传播函数的引用存储在张量的 <code class="docutils literal notranslate"><span class="pre">grad_fn</span></code> 属性中。</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Gradient function for z = </span><span class="si">{</span><span class="n">z</span><span class="o">.</span><span class="n">grad_fn</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Gradient function for loss = </span><span class="si">{</span><span class="n">loss</span><span class="o">.</span><span class="n">grad_fn</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Gradient function for z = &lt;AddBackward0 object at 0x7f25610029e0&gt;
Gradient function for loss = &lt;BinaryCrossEntropyWithLogitsBackward0 object at 0x7f2561002da0&gt;
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="id3">
<h2>计算梯度<a class="headerlink" href="#id3" title="永久链接至标题">¶</a></h2>
<p>为了优化神经网络中参数的权值，需要计算损失函数对参数的导数，即在 <code class="docutils literal notranslate"><span class="pre">x</span></code> 和 <code class="docutils literal notranslate"><span class="pre">y</span></code> 的某些固定值下，需要计算 <span class="math notranslate nohighlight">\(\frac{\partial loss}{\partial b}\)</span> 和 <span class="math notranslate nohighlight">\(\frac{\partial loss}{\partial w}\)</span>。为了计算这些导数，调用 <code class="xref py py-func docutils literal notranslate"><span class="pre">loss.backward()</span></code>，然后从 <code class="docutils literal notranslate"><span class="pre">w.grad</span></code> 和 <code class="docutils literal notranslate"><span class="pre">b.grad</span></code> 检索值：</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">w</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">b</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[0.2963, 0.2867, 0.2677],
        [0.2963, 0.2867, 0.2677],
        [0.2963, 0.2867, 0.2677],
        [0.2963, 0.2867, 0.2677],
        [0.2963, 0.2867, 0.2677]])
tensor([0.2963, 0.2867, 0.2677])
</pre></div>
</div>
</div>
</div>
<div class="alert-info admonition note">
<p class="admonition-title">备注</p>
<ul class="simple">
<li><p>只能获得计算图的叶节点的 <code class="docutils literal notranslate"><span class="pre">grad</span></code> 属性，它们的 <code class="docutils literal notranslate"><span class="pre">requires_grad</span></code> 属性设置为 <code class="docutils literal notranslate"><span class="pre">True</span></code>。对于图中的所有其他节点，梯度将不可用。</p></li>
<li><p>由于性能原因，只能在给定的图上使用一次 <code class="docutils literal notranslate"><span class="pre">backward</span></code> 梯度计算。如果需要对同一个图进行多次 <code class="docutils literal notranslate"><span class="pre">backward</span></code> 调用，则需要将 <code class="docutils literal notranslate"><span class="pre">retain_graph=True</span></code> 传递给 <code class="docutils literal notranslate"><span class="pre">backward</span></code> 调用。</p></li>
</ul>
</div>
</div>
<div class="section" id="id4">
<h2>禁用梯度追踪<a class="headerlink" href="#id4" title="永久链接至标题">¶</a></h2>
<p>默认情况下，所有 <code class="docutils literal notranslate"><span class="pre">requires_grad=True</span></code> 的张量都会跟踪它们的计算历史，并支持梯度计算。但是，在某些情况下，不需要这样做，例如，训练模型后，只是想把它应用到一些输入数据上，即只想通过网络进行正向计算。可以通过使用 <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.no_grad()</span></code> 块包围计算代码来停止跟踪计算：</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">z</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span><span class="o">+</span><span class="n">b</span>
<span class="nb">print</span><span class="p">(</span><span class="n">z</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">)</span>

<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span><span class="o">+</span><span class="n">b</span>
<span class="nb">print</span><span class="p">(</span><span class="n">z</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>True
False
</pre></div>
</div>
</div>
</div>
<p>另一种实现相同结果的方法是对张量使用 <code class="xref py py-meth docutils literal notranslate"><span class="pre">detach()</span></code> 方法：</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">z</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span><span class="o">+</span><span class="n">b</span>
<span class="n">z_det</span> <span class="o">=</span> <span class="n">z</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">z_det</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>False
</pre></div>
</div>
</div>
</div>
<p>以下是禁用梯度跟踪的原因：</p>
<ul class="simple">
<li><p>将神经网络中的一些参数标记为 <strong>冻结参数</strong> （frozen parameters）。这是对<a class="reference external" href="https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html">预训练的网络进行微调</a>的非常常见的场景。</p></li>
<li><p>在只进行正向传播的情况下 <strong>加快计算速度</strong>，因为在不跟踪梯度的张量上的计算将更加有效。</p></li>
</ul>
</div>
<div class="section" id="id5">
<h2>更多关于计算图的内容<a class="headerlink" href="#id5" title="永久链接至标题">¶</a></h2>
<p>从概念上讲，<a class="reference external" href="https://pytorch.org/docs/stable/autograd.html#module-torch.autograd" title="(在 PyTorch v1.11.0)"><code class="xref py py-mod docutils literal notranslate"><span class="pre">torch.autograd</span></code></a> 在由 <a class="reference external" href="https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function" title="(在 PyTorch v1.11.0)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.autograd.Function</span></code></a> 对象组成的有向无环图（DAG）中保存数据（张量）和所有执行的运算（以及产生的新张量）的记录。在这个 DAG 中，叶是输入张量，根是输出张量。通过从根到叶跟踪这个图，可以使用链式法则自动计算梯度。</p>
<p>在forward 传播时，<code class="docutils literal notranslate"><span class="pre">autograd</span></code> 会同时做两件事：</p>
<ul class="simple">
<li><p>运行请求的运算来计算结果张量。</p></li>
<li><p>在 DAG 中维护运算的 <em>梯度函数</em>。</p></li>
</ul>
<p>当在 DAG 根上调用 <code class="docutils literal notranslate"><span class="pre">.backward()</span></code> 时，后向传播开始。然后，<code class="docutils literal notranslate"><span class="pre">autograd</span></code>：从每个 <code class="docutils literal notranslate"><span class="pre">.grad_fn</span></code> 计算梯度，使用链式规则将它们累加到各自张量的 <code class="docutils literal notranslate"><span class="pre">.grad</span></code> 属性中，并一路传播到叶张量。</p>
<div class="alert-info admonition note">
<p class="admonition-title">备注</p>
<p>在 PyTorch 中，DAG 是动态的。在每次 <code class="docutils literal notranslate"><span class="pre">.backward()</span></code> 调用之后，<code class="docutils literal notranslate"><span class="pre">autograd</span></code> 开始填充新的图。这正是允许你在模型中使用控制流语句的原因；如果需要，您可以在每次迭代中更改形状、大小和运算。</p>
</div>
</div>
<div class="section" id="id6">
<h2>选读：张量梯度和雅可比积<a class="headerlink" href="#id6" title="永久链接至标题">¶</a></h2>
<p>在很多情况下，损失函数是标量的，需要计算关于一些参数的梯度。然而，也有输出函数是任意张量的情况。在这种情况下，PyTorch 允许你计算所谓的雅可比乘积（Jacobian product），而不是实际的梯度。</p>
<p>对于向量函数 <span class="math notranslate nohighlight">\(\vec{y}=f(\vec{x})\)</span>，其中 <span class="math notranslate nohighlight">\(\vec{x}=\langle x_1,\dots,x_n\rangle\)</span> 和 <span class="math notranslate nohighlight">\(\vec{y}=\langle y_1,\dots,y_m\rangle\)</span>，<span class="math notranslate nohighlight">\(\vec{y}\)</span> 对 <span class="math notranslate nohighlight">\(\vec{x}\)</span> 的梯度雅可比矩阵：</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}J=\left(\begin{array}{ccc}
      \frac{\partial y_{1}}{\partial x_{1}} &amp; \cdots &amp; \frac{\partial y_{1}}{\partial x_{n}}\\
      \vdots &amp; \ddots &amp; \vdots\\
      \frac{\partial y_{m}}{\partial x_{1}} &amp; \cdots &amp; \frac{\partial y_{m}}{\partial x_{n}}
      \end{array}\right)
\end{align}
\end{split}\]</div>
<p>与计算雅可比矩阵本身不同，PyTorch 允许你为给定的输入向量 <span class="math notranslate nohighlight">\(v=(v_1 \dots v_m)\)</span> 计算雅可比积 <span class="math notranslate nohighlight">\(v^T\cdot J\)</span>。这是通过 <code class="docutils literal notranslate"><span class="pre">backward</span></code> 调用参数 <span class="math notranslate nohighlight">\(v\)</span> 实现的。<span class="math notranslate nohighlight">\(v\)</span> 的大小应该和原始张量的大小一样，要根据它来计算乘积：</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">inp</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">out</span> <span class="o">=</span> <span class="p">(</span><span class="n">inp</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="n">out</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">inp</span><span class="p">),</span> <span class="n">retain_graph</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;First call</span><span class="se">\n</span><span class="si">{</span><span class="n">inp</span><span class="o">.</span><span class="n">grad</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="n">out</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">inp</span><span class="p">),</span> <span class="n">retain_graph</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Second call</span><span class="se">\n</span><span class="si">{</span><span class="n">inp</span><span class="o">.</span><span class="n">grad</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="n">inp</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>
<span class="n">out</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">inp</span><span class="p">),</span> <span class="n">retain_graph</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Call after zeroing gradients</span><span class="se">\n</span><span class="si">{</span><span class="n">inp</span><span class="o">.</span><span class="n">grad</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="admonition caution">
<p class="admonition-title">小心</p>
<p>当使用相同的参数第二次调用 <code class="docutils literal notranslate"><span class="pre">backward</span></code> 时，梯度的值是不同的。这是因为在做反向传播时，PyTorch 会对梯度进行累加，即计算出的梯度的值被添加到计算图的所有叶子节点的 <code class="docutils literal notranslate"><span class="pre">grad</span></code> 属性中。如果你想计算正确的梯度，你需要在此之前将 <code class="docutils literal notranslate"><span class="pre">grad</span></code> 属性归零。在现实训练中，优化器可以帮助做到这一点。</p>
</div>
<div class="alert-info admonition note">
<p class="admonition-title">备注</p>
<p>以前调用的是不带参数的 <code class="xref py py-func docutils literal notranslate"><span class="pre">backward()</span></code> 函数。这本质上相当于调用 <code class="docutils literal notranslate"><span class="pre">backward(torch.tensor(1.0))</span></code>，对于标量值函数（如神经网络训练期间的 loss），这是一种计算梯度的有用方法。</p>
</div>
</div>
</div>


              </div>
              
              
              <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="quickstart.html" title="上一页 页">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">上一页</p>
            <p class="prev-next-title">快速入门</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="../notes/index.html" title="下一页 页">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">下一页</p>
        <p class="prev-next-title">笔记</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
              
          </main>
          

      </div>
    </div>
  
  <script src="../../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>
<footer class="footer mt-5 mt-md-0">
  <div class="container">
    
    <div class="footer-item">
      <p class="copyright">
    &copy; Copyright 2022, xinetzone.<br>
</p>
    </div>
    
    <div class="footer-item">
      <p class="last-updated">
最后更新于 2022-03-22, 10:48:14.<br>
</p>
    </div>
    
    <div class="footer-item">
      <p class="sphinx-version">
Created using <a href="http://sphinx-doc.org/">Sphinx</a> 4.4.0.<br>
</p>
    </div>
    
  </div>
</footer>
  </body>
</html>