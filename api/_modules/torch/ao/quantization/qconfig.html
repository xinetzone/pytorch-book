
<!DOCTYPE html>

<html lang="zh_CN">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>torch.ao.quantization.qconfig &#8212; Pytorch Book 0.0.1 文档</title>
    
  <link href="../../../../_static/css/theme.css" rel="stylesheet">
  <link href="../../../../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../../../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../../../../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/css/style.css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/tabs.css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/default.css" />
    
  <link rel="preload" as="script" href="../../../../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="../../../../" id="documentation_options" src="../../../../_static/documentation_options.js"></script>
    <script src="../../../../_static/jquery.js"></script>
    <script src="../../../../_static/underscore.js"></script>
    <script src="../../../../_static/doctools.js"></script>
    <script src="../../../../_static/clipboard.min.js"></script>
    <script src="../../../../_static/copybutton.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../../../_static/togglebutton.js"></script>
    <script src="../../../../_static/tabs.js"></script>
    <script src="../../../../_static/translations.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <link rel="canonical" href="https://xinetzone.github.io/pytorch-book/_modules/torch/ao/quantization/qconfig.html" />
    <link rel="shortcut icon" href="../../../../_static/favicon.jpg"/>
    <link rel="index" title="索引" href="../../../../genindex.html" />
    <link rel="search" title="搜索" href="../../../../search.html" />
    <link rel="stylesheet" href="../../../../_static/css/default.css"/>
    <link rel="stylesheet" href="../../../../_static/css/custom.css"/>

  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    
    <nav class="navbar navbar-light navbar-expand-lg bg-light fixed-top bd-navbar" id="navbar-main"><div class="container-xl">

  <div id="navbar-start">
    
    

<a class="navbar-brand" href="../../../../index.html">
  <img src="../../../../_static/logo.jpg" class="logo" alt="logo">
</a>


    
  </div>

  <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbar-collapsible" aria-controls="navbar-collapsible" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
  </button>

  
  <div id="navbar-collapsible" class="col-lg-9 collapse navbar-collapse">
    <div id="navbar-center" class="mr-auto">
      
      <div class="navbar-center-item">
        <ul id="navbar-main-elements" class="navbar-nav">
    <li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../../../../hymenoptera.html">
  hymenoptera
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../../../../pytorch/index.html">
  PyTorch API
 </a>
</li>

    
    <li class="nav-item">
        <a class="nav-link nav-external" href="https://xinetzone.github.io/pytorch-book">文档<i class="fas fa-external-link-alt"></i></a>
    </li>
    
</ul>
      </div>
      
    </div>

    <div id="navbar-end">
      
      <div class="navbar-end-item">
        <ul id="navbar-icon-links" class="navbar-nav" aria-label="Icon Links">
        <li class="nav-item">
          <a class="nav-link" href="https://github.com/xinetzone/pytorch-book" rel="noopener" target="_blank" title="GitHub">
            <span><i class="fab fa-github-square"></i></span>
            <label class="sr-only">GitHub</label>
          </a>
        </li>
      </ul>
      </div>
      
    </div>
  </div>
</div>
    </nav>
    

    <div class="container-xl">
      <div class="row">
          
            
            <!-- Only show if we have sidebars configured, else just a small margin  -->
            <div class="col-12 col-md-3 bd-sidebar"><form class="bd-search d-flex align-items-center" action="../../../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
  <div class="bd-toc-item active">
    
  </div>
</nav>
            </div>
            
          

          
          <div class="d-none d-xl-block col-xl-2 bd-toc">
            
          </div>
          

          
          
            
          
          <main class="col-12 col-md-9 col-xl-7 py-md-5 pl-md-5 pr-md-4 bd-content" role="main">
              
              <div>
                
  <h1>torch.ao.quantization.qconfig 源代码</h1><div class="highlight"><pre>
<span></span><span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">namedtuple</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">Any</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">from</span> <span class="nn">torch.ao.quantization.fake_quantize</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">FakeQuantize</span><span class="p">,</span>
    <span class="n">FakeQuantizeBase</span><span class="p">,</span>
    <span class="n">default_fake_quant</span><span class="p">,</span>
    <span class="n">default_dynamic_fake_quant</span><span class="p">,</span>
    <span class="n">default_per_channel_weight_fake_quant</span><span class="p">,</span>
    <span class="n">default_weight_fake_quant</span><span class="p">,</span>
    <span class="n">default_fused_act_fake_quant</span><span class="p">,</span>
    <span class="n">default_fused_wt_fake_quant</span><span class="p">,</span>
    <span class="n">FusedMovingAvgObsFakeQuantize</span><span class="p">,</span>
    <span class="n">default_fused_per_channel_wt_fake_quant</span><span class="p">,</span>
    <span class="n">default_embedding_fake_quant</span><span class="p">,</span>
    <span class="n">default_embedding_fake_quant_4bit</span><span class="p">,</span>
<span class="p">)</span>

<span class="kn">from</span> <span class="nn">.observer</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">HistogramObserver</span><span class="p">,</span>
    <span class="n">MovingAverageMinMaxObserver</span><span class="p">,</span>
    <span class="n">NoopObserver</span><span class="p">,</span>
    <span class="n">PlaceholderObserver</span><span class="p">,</span>
    <span class="n">ReuseInputObserver</span><span class="p">,</span>
    <span class="n">default_debug_observer</span><span class="p">,</span>
    <span class="n">default_dynamic_quant_observer</span><span class="p">,</span>
    <span class="n">default_float_qparams_observer</span><span class="p">,</span>
    <span class="n">default_float_qparams_observer_4bit</span><span class="p">,</span>
    <span class="n">default_observer</span><span class="p">,</span>
    <span class="n">default_per_channel_weight_observer</span><span class="p">,</span>
    <span class="n">default_placeholder_observer</span><span class="p">,</span>
    <span class="n">default_weight_observer</span><span class="p">,</span>
    <span class="n">default_reuse_input_observer</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">import</span> <span class="nn">warnings</span>


<div class="viewcode-block" id="QConfig"><a class="viewcode-back" href="../../../../pytorch/ao/quantization/generated/torch.ao.quantization.qconfig.QConfig.html#torch.ao.quantization.qconfig.QConfig">[文档]</a><span class="k">class</span> <span class="nc">QConfig</span><span class="p">(</span><span class="n">namedtuple</span><span class="p">(</span><span class="s1">&#39;QConfig&#39;</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;activation&#39;</span><span class="p">,</span> <span class="s1">&#39;weight&#39;</span><span class="p">])):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Describes how to quantize a layer or a part of the network by providing</span>
<span class="sd">    settings (observer classes) for activations and weights respectively.</span>


<span class="sd">    Note that QConfig needs to contain observer **classes** (like MinMaxObserver) or a callable that returns</span>
<span class="sd">    instances on invocation, not the concrete observer instances themselves.</span>
<span class="sd">    Quantization preparation function will instantiate observers multiple times for each of the layers.</span>


<span class="sd">    Observer classes have usually reasonable default arguments, but they can be overwritten with `with_args`</span>
<span class="sd">    method (that behaves like functools.partial)::</span>

<span class="sd">      my_qconfig = QConfig(</span>
<span class="sd">          activation=MinMaxObserver.with_args(dtype=torch.qint8),</span>
<span class="sd">          weight=default_observer.with_args(dtype=torch.qint8))</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__new__</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">activation</span><span class="p">,</span> <span class="n">weight</span><span class="p">):</span>
        <span class="c1"># catch common mistakes</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">activation</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">)</span> <span class="ow">or</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">weight</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;QConfig received observer instance, please pass observer class instead. &quot;</span> <span class="o">+</span>
                             <span class="s2">&quot;Use MyObserver.with_args(x=1) to override arguments to constructor if needed&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="nb">super</span><span class="p">(</span><span class="n">QConfig</span><span class="p">,</span> <span class="bp">cls</span><span class="p">)</span><span class="o">.</span><span class="fm">__new__</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">activation</span><span class="p">,</span> <span class="n">weight</span><span class="p">)</span></div>


<div class="viewcode-block" id="QConfigDynamic"><a class="viewcode-back" href="../../../../pytorch/ao/quantization/generated/torch.ao.quantization.qconfig.QConfigDynamic.html#torch.ao.quantization.qconfig.QConfigDynamic">[文档]</a><span class="k">class</span> <span class="nc">QConfigDynamic</span><span class="p">(</span><span class="n">namedtuple</span><span class="p">(</span><span class="s1">&#39;QConfigDynamic&#39;</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;activation&#39;</span><span class="p">,</span> <span class="s1">&#39;weight&#39;</span><span class="p">])):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Describes how to dynamically quantize a layer or a part of the network by providing</span>
<span class="sd">    settings (observer classes) for weights.</span>

<span class="sd">    It&#39;s like QConfig, but for dynamic quantization.</span>

<span class="sd">    Note that QConfigDynamic needs to contain observer **classes** (like MinMaxObserver) or a callable that returns</span>
<span class="sd">    instances on invocation, not the concrete observer instances themselves.</span>
<span class="sd">    Quantization function will instantiate observers multiple times for each of the layers.</span>

<span class="sd">    Observer classes have usually reasonable default arguments, but they can be overwritten with `with_args`</span>
<span class="sd">    method (that behaves like functools.partial)::</span>

<span class="sd">      my_qconfig = QConfigDynamic(weight=default_observer.with_args(dtype=torch.qint8))</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__new__</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Identity</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Identity</span><span class="p">):</span>
        <span class="c1"># catch common mistakes</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">weight</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;QConfigDynamic received observer instance, please pass observer class instead. &quot;</span> <span class="o">+</span>
                             <span class="s2">&quot;Use MyObserver.with_args(x=1) to override arguments to constructor if needed&quot;</span><span class="p">)</span>
        <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;QConfigDynamic is going to be deprecated in PyTorch 1.12, please use QConfig instead&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="nb">super</span><span class="p">(</span><span class="n">QConfigDynamic</span><span class="p">,</span> <span class="bp">cls</span><span class="p">)</span><span class="o">.</span><span class="fm">__new__</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">activation</span><span class="p">,</span> <span class="n">weight</span><span class="p">)</span></div>


<span class="n">default_qconfig</span> <span class="o">=</span> <span class="n">QConfig</span><span class="p">(</span><span class="n">activation</span><span class="o">=</span><span class="n">default_observer</span><span class="p">,</span>
                          <span class="n">weight</span><span class="o">=</span><span class="n">default_weight_observer</span><span class="p">)</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Default qconfig configuration.</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="n">default_debug_qconfig</span> <span class="o">=</span> <span class="n">QConfig</span><span class="p">(</span><span class="n">weight</span><span class="o">=</span><span class="n">default_weight_observer</span><span class="p">,</span>
                                <span class="n">activation</span><span class="o">=</span><span class="n">default_debug_observer</span><span class="p">)</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Default qconfig configuration for debugging.</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="n">default_per_channel_qconfig</span> <span class="o">=</span> <span class="n">QConfig</span><span class="p">(</span><span class="n">activation</span><span class="o">=</span><span class="n">default_observer</span><span class="p">,</span>
                                      <span class="n">weight</span><span class="o">=</span><span class="n">default_per_channel_weight_observer</span><span class="p">)</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Default qconfig configuration for per channel weight quantization.</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="n">default_dynamic_qconfig</span> <span class="o">=</span> <span class="n">QConfig</span><span class="p">(</span><span class="n">activation</span><span class="o">=</span><span class="n">default_dynamic_quant_observer</span><span class="p">,</span>
                                  <span class="n">weight</span><span class="o">=</span><span class="n">default_weight_observer</span><span class="p">)</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Default dynamic qconfig.</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="n">float16_dynamic_qconfig</span> <span class="o">=</span> <span class="n">QConfig</span><span class="p">(</span><span class="n">activation</span><span class="o">=</span><span class="n">PlaceholderObserver</span><span class="o">.</span><span class="n">with_args</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span>
                                  <span class="n">weight</span><span class="o">=</span><span class="n">PlaceholderObserver</span><span class="o">.</span><span class="n">with_args</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">))</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Dynamic qconfig with weights quantized to `torch.float16`.</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="n">float16_static_qconfig</span> <span class="o">=</span> <span class="n">QConfig</span><span class="p">(</span><span class="n">activation</span><span class="o">=</span><span class="n">PlaceholderObserver</span><span class="o">.</span><span class="n">with_args</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">),</span>
                                 <span class="n">weight</span><span class="o">=</span><span class="n">PlaceholderObserver</span><span class="o">.</span><span class="n">with_args</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">))</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Dynamic qconfig with both activations and weights quantized to `torch.float16`.</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="n">per_channel_dynamic_qconfig</span> <span class="o">=</span> <span class="n">QConfig</span><span class="p">(</span><span class="n">activation</span><span class="o">=</span><span class="n">default_dynamic_quant_observer</span><span class="p">,</span>
                                      <span class="n">weight</span><span class="o">=</span><span class="n">default_per_channel_weight_observer</span><span class="p">)</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Dynamic qconfig with weights quantized per channel.</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="n">float_qparams_weight_only_qconfig</span> <span class="o">=</span> <span class="n">QConfig</span><span class="p">(</span>
    <span class="n">activation</span><span class="o">=</span><span class="n">default_placeholder_observer</span><span class="p">,</span>
    <span class="n">weight</span><span class="o">=</span><span class="n">default_float_qparams_observer</span><span class="p">)</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Dynamic qconfig with weights quantized with a floating point zero_point.</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="n">float_qparams_weight_only_qconfig_4bit</span> <span class="o">=</span> <span class="n">QConfig</span><span class="p">(</span>
    <span class="n">activation</span><span class="o">=</span><span class="n">default_placeholder_observer</span><span class="p">,</span>
    <span class="n">weight</span><span class="o">=</span><span class="n">default_float_qparams_observer_4bit</span><span class="p">)</span>

<span class="n">default_qat_qconfig</span> <span class="o">=</span> <span class="n">QConfig</span><span class="p">(</span><span class="n">activation</span><span class="o">=</span><span class="n">default_fake_quant</span><span class="p">,</span>
                              <span class="n">weight</span><span class="o">=</span><span class="n">default_weight_fake_quant</span><span class="p">)</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Default qconfig for QAT.</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="n">default_dynamic_qat_qconfig</span> <span class="o">=</span> <span class="n">QConfig</span><span class="p">(</span><span class="n">activation</span><span class="o">=</span><span class="n">default_dynamic_fake_quant</span><span class="p">,</span>
                                      <span class="n">weight</span><span class="o">=</span><span class="n">default_weight_fake_quant</span><span class="p">)</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Default qconfig for dynamic QAT.</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="n">default_weight_only_qconfig</span> <span class="o">=</span> <span class="n">QConfig</span><span class="p">(</span><span class="n">activation</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Identity</span><span class="p">,</span>
                                      <span class="n">weight</span><span class="o">=</span><span class="n">default_weight_fake_quant</span><span class="p">)</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Default qconfig for quantizing weights only.</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="n">default_activation_only_qconfig</span> <span class="o">=</span> <span class="n">QConfig</span><span class="p">(</span><span class="n">activation</span><span class="o">=</span><span class="n">default_fake_quant</span><span class="p">,</span>
                                          <span class="n">weight</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Identity</span><span class="p">)</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Default qconfig for quantizing activations only.</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="c1"># QAT config that uses a fused observer + fake quant modules for optimized training performance.</span>
<span class="c1"># to modify the activation/weight observers, the default entries in fake_quantize.py can be modified.</span>
<span class="n">default_qat_qconfig_v2</span> <span class="o">=</span> <span class="n">QConfig</span><span class="p">(</span><span class="n">activation</span><span class="o">=</span><span class="n">default_fused_act_fake_quant</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="n">default_fused_wt_fake_quant</span><span class="p">)</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Fused version of `default_qat_config`, has performance benefits.</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="n">default_reuse_input_qconfig</span> <span class="o">=</span> <span class="n">QConfig</span><span class="p">(</span><span class="n">activation</span><span class="o">=</span><span class="n">default_reuse_input_observer</span><span class="p">,</span>
                                      <span class="n">weight</span><span class="o">=</span><span class="n">NoopObserver</span><span class="p">)</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Default qconfig for operators that reuse the observers from input Tensor, e.g. reshape</span>
<span class="sd">&quot;&quot;&quot;</span>

<div class="viewcode-block" id="get_default_qconfig"><a class="viewcode-back" href="../../../../pytorch/ao/quantization/generated/torch.ao.quantization.qconfig.get_default_qconfig.html#torch.ao.quantization.qconfig.get_default_qconfig">[文档]</a><span class="k">def</span> <span class="nf">get_default_qconfig</span><span class="p">(</span><span class="n">backend</span><span class="o">=</span><span class="s1">&#39;fbgemm&#39;</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the default PTQ qconfig for the specified backend.</span>

<span class="sd">    Args:</span>
<span class="sd">      * `backend`: a string representing the target backend. Currently supports `fbgemm`</span>
<span class="sd">        and `qnnpack`.</span>

<span class="sd">    Return:</span>
<span class="sd">        qconfig</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="n">backend</span> <span class="o">==</span> <span class="s1">&#39;fbgemm&#39;</span><span class="p">:</span>
        <span class="n">qconfig</span> <span class="o">=</span> <span class="n">QConfig</span><span class="p">(</span><span class="n">activation</span><span class="o">=</span><span class="n">HistogramObserver</span><span class="o">.</span><span class="n">with_args</span><span class="p">(</span><span class="n">reduce_range</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
                          <span class="n">weight</span><span class="o">=</span><span class="n">default_per_channel_weight_observer</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">backend</span> <span class="o">==</span> <span class="s1">&#39;qnnpack&#39;</span><span class="p">:</span>
        <span class="n">qconfig</span> <span class="o">=</span> <span class="n">QConfig</span><span class="p">(</span><span class="n">activation</span><span class="o">=</span><span class="n">HistogramObserver</span><span class="o">.</span><span class="n">with_args</span><span class="p">(</span><span class="n">reduce_range</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
                          <span class="n">weight</span><span class="o">=</span><span class="n">default_weight_observer</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">qconfig</span> <span class="o">=</span> <span class="n">default_qconfig</span>
    <span class="k">return</span> <span class="n">qconfig</span></div>

<span class="n">default_embedding_qat_qconfig</span> <span class="o">=</span> <span class="n">QConfig</span><span class="p">(</span><span class="n">activation</span><span class="o">=</span><span class="n">NoopObserver</span><span class="o">.</span><span class="n">with_args</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span>
                                        <span class="n">weight</span><span class="o">=</span><span class="n">default_embedding_fake_quant</span><span class="p">)</span>

<span class="n">default_embedding_qat_qconfig_4bit</span> <span class="o">=</span> <span class="n">QConfig</span><span class="p">(</span><span class="n">activation</span><span class="o">=</span><span class="n">NoopObserver</span><span class="o">.</span><span class="n">with_args</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span>
                                             <span class="n">weight</span><span class="o">=</span><span class="n">default_embedding_fake_quant_4bit</span><span class="p">)</span>

<div class="viewcode-block" id="get_default_qat_qconfig"><a class="viewcode-back" href="../../../../pytorch/ao/quantization/generated/torch.ao.quantization.qconfig.get_default_qat_qconfig.html#torch.ao.quantization.qconfig.get_default_qat_qconfig">[文档]</a><span class="k">def</span> <span class="nf">get_default_qat_qconfig</span><span class="p">(</span><span class="n">backend</span><span class="o">=</span><span class="s1">&#39;fbgemm&#39;</span><span class="p">,</span> <span class="n">version</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the default QAT qconfig for the specified backend.</span>

<span class="sd">    Args:</span>
<span class="sd">      * `backend`: a string representing the target backend. Currently supports `fbgemm`</span>
<span class="sd">        and `qnnpack`.</span>
<span class="sd">      * `version`: version, for backwards compatibility. Can be `None` or `1`.</span>

<span class="sd">    Return:</span>
<span class="sd">        qconfig</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Histogram observer is too slow for quantization aware training</span>
    <span class="k">if</span> <span class="n">version</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">backend</span> <span class="o">==</span> <span class="s1">&#39;fbgemm&#39;</span><span class="p">:</span>
            <span class="n">qconfig</span> <span class="o">=</span> <span class="n">QConfig</span><span class="p">(</span><span class="n">activation</span><span class="o">=</span><span class="n">FakeQuantize</span><span class="o">.</span><span class="n">with_args</span><span class="p">(</span><span class="n">observer</span><span class="o">=</span><span class="n">MovingAverageMinMaxObserver</span><span class="p">,</span>
                                                                <span class="n">quant_min</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                                                                <span class="n">quant_max</span><span class="o">=</span><span class="mi">255</span><span class="p">,</span>
                                                                <span class="n">reduce_range</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
                              <span class="n">weight</span><span class="o">=</span><span class="n">default_per_channel_weight_fake_quant</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">backend</span> <span class="o">==</span> <span class="s1">&#39;qnnpack&#39;</span><span class="p">:</span>
            <span class="n">qconfig</span> <span class="o">=</span> <span class="n">QConfig</span><span class="p">(</span><span class="n">activation</span><span class="o">=</span><span class="n">FakeQuantize</span><span class="o">.</span><span class="n">with_args</span><span class="p">(</span><span class="n">observer</span><span class="o">=</span><span class="n">MovingAverageMinMaxObserver</span><span class="p">,</span>
                                                                <span class="n">quant_min</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                                                                <span class="n">quant_max</span><span class="o">=</span><span class="mi">255</span><span class="p">,</span>
                                                                <span class="n">reduce_range</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
                              <span class="n">weight</span><span class="o">=</span><span class="n">default_weight_fake_quant</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">qconfig</span> <span class="o">=</span> <span class="n">default_qat_qconfig</span>
    <span class="c1"># Use the fused observer + fake_quant modules for doing QAT.</span>
    <span class="k">if</span> <span class="n">version</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">backend</span> <span class="o">==</span> <span class="s1">&#39;fbgemm&#39;</span><span class="p">:</span>
            <span class="n">qconfig</span> <span class="o">=</span> <span class="n">QConfig</span><span class="p">(</span><span class="n">activation</span><span class="o">=</span><span class="n">FusedMovingAvgObsFakeQuantize</span><span class="o">.</span><span class="n">with_args</span><span class="p">(</span><span class="n">observer</span><span class="o">=</span><span class="n">MovingAverageMinMaxObserver</span><span class="p">,</span>
                                                                                 <span class="n">quant_min</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                                                                                 <span class="n">quant_max</span><span class="o">=</span><span class="mi">255</span><span class="p">,</span>
                                                                                 <span class="n">reduce_range</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
                              <span class="n">weight</span><span class="o">=</span><span class="n">default_fused_per_channel_wt_fake_quant</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">backend</span> <span class="o">==</span> <span class="s1">&#39;qnnpack&#39;</span><span class="p">:</span>
            <span class="n">qconfig</span> <span class="o">=</span> <span class="n">QConfig</span><span class="p">(</span><span class="n">activation</span><span class="o">=</span><span class="n">FusedMovingAvgObsFakeQuantize</span><span class="o">.</span><span class="n">with_args</span><span class="p">(</span><span class="n">observer</span><span class="o">=</span><span class="n">MovingAverageMinMaxObserver</span><span class="p">,</span>
                                                                                 <span class="n">quant_min</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                                                                                 <span class="n">quant_max</span><span class="o">=</span><span class="mi">255</span><span class="p">,</span>
                                                                                 <span class="n">reduce_range</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
                              <span class="n">weight</span><span class="o">=</span><span class="n">default_fused_wt_fake_quant</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">qconfig</span> <span class="o">=</span> <span class="n">default_qat_qconfig_v2</span>
    <span class="k">return</span> <span class="n">qconfig</span></div>

<span class="k">def</span> <span class="nf">get_default_qconfig_dict</span><span class="p">(</span><span class="n">backend</span><span class="o">=</span><span class="s1">&#39;fbgemm&#39;</span><span class="p">,</span> <span class="n">version</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
    <span class="n">qconfig</span> <span class="o">=</span> <span class="n">get_default_qconfig</span><span class="p">(</span><span class="n">backend</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">{</span>
        <span class="s2">&quot;&quot;</span><span class="p">:</span> <span class="n">qconfig</span><span class="p">,</span>
        <span class="s2">&quot;object_type&quot;</span><span class="p">:</span> <span class="p">[(</span><span class="s2">&quot;reshape&quot;</span><span class="p">,</span> <span class="n">default_reuse_input_qconfig</span><span class="p">)]</span>
    <span class="p">}</span>

<span class="k">def</span> <span class="nf">get_default_qat_qconfig_dict</span><span class="p">(</span><span class="n">backend</span><span class="o">=</span><span class="s1">&#39;fbgemm&#39;</span><span class="p">,</span> <span class="n">version</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
    <span class="n">qconfig</span> <span class="o">=</span> <span class="n">get_default_qat_qconfig</span><span class="p">(</span><span class="n">backend</span><span class="p">,</span> <span class="n">version</span><span class="o">=</span><span class="n">version</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">{</span>
        <span class="s2">&quot;&quot;</span><span class="p">:</span> <span class="n">qconfig</span><span class="p">,</span>
        <span class="s2">&quot;object_type&quot;</span><span class="p">:</span> <span class="p">[(</span><span class="s2">&quot;reshape&quot;</span><span class="p">,</span> <span class="n">default_reuse_input_qconfig</span><span class="p">)]</span>
    <span class="p">}</span>

<span class="k">def</span> <span class="nf">assert_valid_qconfig</span><span class="p">(</span><span class="n">qconfig</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">QConfig</span><span class="p">],</span>
                         <span class="n">mod</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Verifies that this `qconfig` is valid.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">qconfig</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">return</span>
    <span class="n">is_conv_transpose_mod</span> <span class="o">=</span> <span class="p">(</span>
        <span class="nb">isinstance</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ConvTranspose1d</span><span class="p">)</span> <span class="ow">or</span>
        <span class="nb">isinstance</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ConvTranspose2d</span><span class="p">)</span> <span class="ow">or</span>
        <span class="nb">isinstance</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ConvTranspose3d</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">is_conv_transpose_mod</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">qconfig</span><span class="o">.</span><span class="n">weight</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># for now, we assume that any qconfig for ConvTranspose without a weight is valid</span>
            <span class="k">return</span>
        <span class="n">example_observer</span> <span class="o">=</span> <span class="n">qconfig</span><span class="o">.</span><span class="n">weight</span><span class="p">()</span>
        <span class="n">is_per_channel</span> <span class="o">=</span> <span class="p">(</span>
            <span class="nb">isinstance</span><span class="p">(</span><span class="n">example_observer</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">ao</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">PerChannelMinMaxObserver</span><span class="p">)</span> <span class="ow">or</span>
            <span class="nb">isinstance</span><span class="p">(</span><span class="n">example_observer</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">ao</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">MovingAveragePerChannelMinMaxObserver</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="k">assert</span> <span class="ow">not</span> <span class="n">is_per_channel</span><span class="p">,</span> \
            <span class="s1">&#39;Per channel weight observer is not supported yet for ConvTranspose</span><span class="si">{n}</span><span class="s1">d.&#39;</span>

<span class="c1"># TODO: remove QConfigAny and replace it with Optional[QConfig]</span>
<span class="n">QConfigAny</span> <span class="o">=</span> <span class="n">Optional</span><span class="p">[</span><span class="n">QConfig</span><span class="p">]</span>

<span class="k">def</span> <span class="nf">add_module_to_qconfig_obs_ctr</span><span class="p">(</span>
        <span class="n">qconfig</span><span class="p">:</span> <span class="n">QConfigAny</span><span class="p">,</span>
        <span class="n">module</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Any</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;This is a helper function for use in quantization prepare that updates a qconfig so that</span>
<span class="sd">    the constructors stored in the qconfig will create observers on the same device that</span>
<span class="sd">    &#39;module&#39; is on. This is intended to be used when the qconfigs are propagated to each</span>
<span class="sd">    module in order to avoid potential device alignment issues.</span>

<span class="sd">    Args:</span>
<span class="sd">        qconfig: QConfig with obs constructors stored in activation and weight</span>
<span class="sd">        module: module which the qconfig is related to</span>

<span class="sd">    Return:</span>
<span class="sd">        qconfig: configured so that obs constructors set to construct on the same device as module</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="n">module</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">qconfig</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">qconfig</span><span class="o">.</span><span class="n">_fields</span> <span class="o">!=</span> <span class="p">(</span><span class="s1">&#39;activation&#39;</span><span class="p">,</span> <span class="s1">&#39;weight&#39;</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">qconfig</span>

    <span class="k">def</span> <span class="nf">get_factory_kwargs_based_on_module_device</span><span class="p">():</span>
        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">)</span>
        <span class="n">devices</span> <span class="o">=</span> <span class="p">{</span><span class="n">p</span><span class="o">.</span><span class="n">device</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">module</span><span class="o">.</span><span class="n">parameters</span><span class="p">()}</span> <span class="o">|</span> \
            <span class="p">{</span><span class="n">p</span><span class="o">.</span><span class="n">device</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">module</span><span class="o">.</span><span class="n">buffers</span><span class="p">()}</span>
        <span class="n">device</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">devices</span><span class="p">))</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">devices</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="kc">None</span>
        <span class="k">return</span> <span class="kc">None</span> <span class="k">if</span> <span class="n">device</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="p">{</span><span class="s1">&#39;device&#39;</span><span class="p">:</span> <span class="n">device</span><span class="p">}</span>

    <span class="k">def</span> <span class="nf">configure_constructor_to_put_obs_on_module_device</span><span class="p">(</span><span class="n">original_constructor</span><span class="p">):</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="c1"># check if constructor can accept factory_kwargs</span>
            <span class="n">check</span> <span class="o">=</span> <span class="n">original_constructor</span><span class="o">.</span><span class="n">with_args</span><span class="p">(</span><span class="n">factory_kwargs</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
            <span class="n">check</span><span class="p">()</span>
            <span class="k">return</span> <span class="n">original_constructor</span><span class="o">.</span><span class="n">with_callable_args</span><span class="p">(</span><span class="n">factory_kwargs</span><span class="o">=</span><span class="n">get_factory_kwargs_based_on_module_device</span><span class="p">)</span>
        <span class="k">except</span> <span class="ne">AttributeError</span><span class="p">:</span>  <span class="c1"># qconfig doesn&#39;t have activation or weight</span>
            <span class="k">return</span> <span class="n">original_constructor</span>
        <span class="k">except</span> <span class="ne">TypeError</span><span class="p">:</span>  <span class="c1"># the class doesn&#39;t accept factory_kwargs argument</span>
            <span class="k">return</span> <span class="n">original_constructor</span>

    <span class="n">activation</span> <span class="o">=</span> <span class="n">configure_constructor_to_put_obs_on_module_device</span><span class="p">(</span><span class="n">qconfig</span><span class="o">.</span><span class="n">activation</span><span class="p">)</span>
    <span class="n">weight</span> <span class="o">=</span> <span class="n">configure_constructor_to_put_obs_on_module_device</span><span class="p">(</span><span class="n">qconfig</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">QConfig</span><span class="p">(</span><span class="n">activation</span><span class="p">,</span> <span class="n">weight</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">qconfig_equals</span><span class="p">(</span><span class="n">q1</span><span class="p">:</span> <span class="n">QConfigAny</span><span class="p">,</span> <span class="n">q2</span><span class="p">:</span> <span class="n">QConfigAny</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns `True` if `q1` equals `q2`, and `False` otherwise.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># functools.partial has no __eq__ operator defined so &#39;==&#39; defaults to &#39;is&#39;</span>
    <span class="k">def</span> <span class="nf">partial_equals</span><span class="p">(</span><span class="n">p1</span><span class="p">,</span> <span class="n">p2</span><span class="p">):</span>
        <span class="n">same</span> <span class="o">=</span> <span class="n">p1</span><span class="o">.</span><span class="n">func</span> <span class="o">==</span> <span class="n">p2</span><span class="o">.</span><span class="n">func</span>
        <span class="n">same</span> <span class="o">=</span> <span class="n">same</span> <span class="ow">and</span> <span class="n">p1</span><span class="o">.</span><span class="n">args</span> <span class="o">==</span> <span class="n">p2</span><span class="o">.</span><span class="n">args</span>
        <span class="k">return</span> <span class="n">same</span> <span class="ow">and</span> <span class="n">p1</span><span class="o">.</span><span class="n">keywords</span> <span class="o">==</span> <span class="n">p2</span><span class="o">.</span><span class="n">keywords</span>

    <span class="k">if</span> <span class="n">q1</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">q2</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">q1</span> <span class="o">==</span> <span class="n">q2</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">assert</span> <span class="n">q1</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">q2</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="c1"># Qconfig weight and activation can be either a partial wrapper,</span>
            <span class="c1"># or an observer class. Special handling is required (above) for</span>
            <span class="c1"># comparing partial wrappers.</span>
            <span class="k">if</span><span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">q1</span><span class="o">.</span><span class="n">activation</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">ao</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">observer</span><span class="o">.</span><span class="n">_PartialWrapper</span><span class="p">)):</span>
                <span class="n">activation_same</span> <span class="o">=</span> <span class="n">partial_equals</span><span class="p">(</span><span class="n">q1</span><span class="o">.</span><span class="n">activation</span><span class="o">.</span><span class="n">p</span><span class="p">,</span> <span class="n">q2</span><span class="o">.</span><span class="n">activation</span><span class="o">.</span><span class="n">p</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">activation_same</span> <span class="o">=</span> <span class="n">q1</span><span class="o">.</span><span class="n">activation</span> <span class="o">==</span> <span class="n">q2</span><span class="o">.</span><span class="n">activation</span>
            <span class="k">if</span><span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">q1</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">ao</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">observer</span><span class="o">.</span><span class="n">_PartialWrapper</span><span class="p">)):</span>
                <span class="n">weight_same</span> <span class="o">=</span> <span class="n">partial_equals</span><span class="p">(</span><span class="n">q1</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">p</span><span class="p">,</span> <span class="n">q2</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">p</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">weight_same</span> <span class="o">=</span> <span class="n">q1</span><span class="o">.</span><span class="n">weight</span> <span class="o">==</span> <span class="n">q2</span><span class="o">.</span><span class="n">weight</span>

            <span class="k">return</span> <span class="n">activation_same</span> <span class="ow">and</span> <span class="n">weight_same</span>
        <span class="k">except</span> <span class="ne">AttributeError</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">q1</span> <span class="o">==</span> <span class="n">q2</span>

<span class="k">def</span> <span class="nf">activation_is_memoryless</span><span class="p">(</span><span class="n">qconfig</span><span class="p">:</span> <span class="n">QConfig</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Return whether the observer for activations defined in the given QConfig is memoryless.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">_is_memoryless</span><span class="p">(</span><span class="n">observer</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">observer</span><span class="p">,</span> <span class="s2">&quot;memoryless&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="n">observer</span><span class="o">.</span><span class="n">memoryless</span>
    <span class="n">act</span> <span class="o">=</span> <span class="n">qconfig</span><span class="o">.</span><span class="n">activation</span><span class="p">()</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">act</span><span class="p">,</span> <span class="n">FakeQuantizeBase</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">act</span><span class="p">,</span> <span class="s2">&quot;activation_post_process&quot;</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">_is_memoryless</span><span class="p">(</span><span class="n">act</span><span class="o">.</span><span class="n">activation_post_process</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">_is_memoryless</span><span class="p">(</span><span class="n">act</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">is_reuse_input_qconfig</span><span class="p">(</span><span class="n">qconfig</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">QConfig</span><span class="p">]):</span>
    <span class="k">return</span> <span class="n">qconfig</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> \
        <span class="nb">isinstance</span><span class="p">(</span><span class="n">qconfig</span><span class="o">.</span><span class="n">activation</span><span class="p">(),</span> <span class="n">ReuseInputObserver</span><span class="p">)</span> <span class="ow">and</span> \
        <span class="nb">isinstance</span><span class="p">(</span><span class="n">qconfig</span><span class="o">.</span><span class="n">weight</span><span class="p">(),</span> <span class="n">NoopObserver</span><span class="p">)</span>
</pre></div>

              </div>
              
              
              <!-- Previous / next buttons -->
<div class='prev-next-area'>
</div>
              
          </main>
          

      </div>
    </div>
  
  <script src="../../../../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>
<footer class="footer mt-5 mt-md-0">
  <div class="container">
    
    <div class="footer-item">
      <p class="copyright">
    &copy; Copyright 2022, xinetzone.<br>
</p>
    </div>
    
    <div class="footer-item">
      <p class="last-updated">
最后更新于 2022-03-18, 12:46:31.<br>
</p>
    </div>
    
    <div class="footer-item">
      <p class="sphinx-version">
Created using <a href="http://sphinx-doc.org/">Sphinx</a> 4.4.0.<br>
</p>
    </div>
    
  </div>
</footer>
  </body>
</html>